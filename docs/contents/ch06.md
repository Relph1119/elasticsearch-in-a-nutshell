# 第6章 Elasticsearch分词

## 6.1 分词基本概念

- 分词发生的阶段：写入数据阶段、执行检索阶段
- 分词器的组成：字符过滤器`filter`、分词器`tokenizer`、分词后再过滤

## 6.2 IK中文分词器

- 使用原因：
    - IK分为细粒度（`ik_max_word`）和粗粒度（`ik_smart`）
    - IK更新字典时，只需要在词典末尾添加关键词即可。
    - IK分词插件与Elasticsearch版本保存高度一致。
- 注意事项：
    - IK自带词典并不完备，建议结合业务添加自定义词典。
    - IK采用动态添加词典的方式，建议修改IK分词插件源码，与MySQL数据库结合。
    
## 6.3 Ngram分词器

- Ngram分词定义：将文本里面的内容按照字节大小进行滑动窗口操作，形成长度是N的字节片段序列。
- 应用场景：
    - 文本压缩、检查拼写错误、加速字符串查找、文献语种识别。
    - NLP自动化
    - 自然语言的自动分类功能
- 分词选型上的注意事项：
    - 若数据量非常少且不要求字串高亮，可以考虑`keyword`。
    - 若数据量大且要求字串高亮，使用`Ngram`分词，结合`match`或者`match_phrase`检索实现。
    - 若数据量大，则不建议使用`wildcard`前缀匹配。